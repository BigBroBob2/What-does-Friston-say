## G-Density: how brain encodes beliefs about world causes

Approximation of VFE, the so-called Laplace-encoded energy $E(\mu,\varphi)$ is over approximate G-density $p(\mu,\varphi)$, which replaces world states $\vartheta$ with sufficient statistical mean $\mu$.

Here we build generative models of world causal dependencies in world and relation to sensory data. Then from generative models we derive specification of G-density.

### The simplest generative model

Definition table

|Symbol|Name \& Description|
|:-|:-|
|$g(\mu;\vartheta)$         |generative mapping     |
|$z$                        |Guassian noise, $N(0,\sigma_z)$       |
|$w$                        |Guassian noise, $N(0,\sigma_w)$       |
|$p(\varphi \mid \mu)$      |likelihood of $\mu$, implicit beliefs about how agent belief about world state $\mu$ map to sensory input $\varphi$|
|$p(\mu)$                   |*prior* of $\mu$, before sensory input $\varphi$|

Simplest model:
- single world state $\vartheta$ and single sensory channel
- agent uses single brain state $\mu$ and sensory input $\varphi$

Agent has generative belief of sensory input $\varphi$:

$\varphi = g(\mu;\vartheta)+z$

Where $g$ is a function and $z$ is zero-mean white noise. This means agent believes a mapping from world state (here denoted as its belief about world state $\mu$) to sensory input.

Assume the agent belief about world state is generated by:

$\mu = \bar{\mu}+w$

Where $\bar{\mu}$ is fixed parameter and $w$ is zero-mean white noise. This means agent takes belief of world states history-independently, fluctuating around $\bar{\mu}$ as *priori*. 

Note that $\mu$ itself represents mean value of world state $\vartheta$, although generated from a distribution. There is a conflict that the best estimated $\mu$, which is $q(\vartheta)$-average $\int \vartheta q(\vartheta) d\vartheta$, may not necessarily be the same as the world model expectation $\bar{\mu}$. 

Given the Guassian distribution of $z$:

$p(z) = \frac{1}{\sqrt{2\pi\sigma_z}} \exp \left ( - \frac{z^2}{2\sigma_z}\right )$

Substitute $z=\varphi-g(\mu;\vartheta)$ and we get $p(\varphi|\mu)$:

$p(\varphi|\mu) = \frac{1}{\sqrt{2\pi\sigma_z}} \exp \left ( - \frac{(\varphi-g(\mu;\vartheta))^2}{2\sigma_z}\right )$

Also we have $p(\mu)$:

$p(\mu) = \frac{1}{\sqrt{2\pi\sigma_w}} \exp \left ( - \frac{(\mu-\bar{\mu})^2}{2\sigma_w}\right )$

The Laplace-encoded G-density $p(\mu,\varphi)$:

$p(\mu,\varphi) = p(\varphi \mid \mu)p(\mu)$

The Laplace-encoded energy $E(\mu,\varphi)$:

$E(\mu,\varphi) = - \ln p(\varphi \mid \mu) - \ln p(\mu) = \frac{1}{2\sigma_z} (\varphi-g(\mu;\vartheta))^2+ \frac{1}{2\sigma_w} (\mu-\bar{\mu})^2 + \frac{1}{2} \ln (\sigma_z \sigma_w) \equiv \frac{1}{2}\sigma_z \varepsilon_z^2+ \frac{1}{2}\sigma_w \varepsilon_w^2 + \frac{1}{2} \ln (\sigma_z \sigma_w)$

Where in predictive coding terminology:
- $\varepsilon_z = \frac{\mu-\bar{\mu}}{\sigma_z}$ is called **residual error**, sensory prediction error;
- $\varepsilon_w = \frac{\varphi-g(\mu;\vartheta)}{\sigma_w}$ is called **prediction error**, model prediction error;
- Both errors have weighted coefficients, how they contribute to the Laplace-encoded energy.

Extend from univariate to multivariate case. For vector of $N$ brain states $\{\mu_\alpha\}$:

$\mu_\alpha = \bar{\mu}_\alpha+w_\alpha$

Note that here noise sources $\{w_\alpha\}$ may be correlated and not independent:

$\vec{w} \sim N \left ( \vec{0},\Sigma_w \right )$

The vector of $N$ snesory inputs $\{\varphi_\alpha\}$:

$\varphi_\alpha = g_\alpha (\mu_0,\dots, \mu_N) + z_\alpha$

Here noise sources $\{w_z\}$ may be correlated and not independent:

$\vec{z} \sim N \left ( \vec{0},\Sigma_z \right )$

The *prior* of brain states:

$p(\vec{\mu}) = \frac{1}{\sqrt{(2\pi)^N | \Sigma_w |}} \exp \left ( -\frac{1}{2}(\vec{\mu}-\vec{\bar{\mu}})^T \Sigma_w^{-1} (\vec{\mu}-\vec{\bar{\mu}}) \right )$

The likelihood density:

$p(\vec{\varphi} \mid \vec{\mu}) = \frac{1}{\sqrt{(2\pi)^N | \Sigma_z |}} \exp \left ( -\frac{1}{2}(\vec{\varphi}-g(\vec{\mu}))^T \Sigma_z^{-1} (\vec{\varphi}-g(\vec{\mu})) \right )$

The Laplace-encoded energy:

$E(\mu,\varphi) = \frac{1}{2} (\vec{\mu}-\vec{\bar{\mu}})^T \Sigma_w^{-1} (\vec{\mu}-\vec{\bar{\mu}}) + \frac{1}{2} (\vec{\varphi}-g(\vec{\mu}))^T \Sigma_z^{-1} (\vec{\varphi}-g(\vec{\mu})) + \frac{1}{2} \ln | \Sigma_z | +  \frac{1}{2} \ln | \Sigma_w |$

Where in predictive coding terminology:
- $\vec{\varepsilon}_z = \Sigma_w^{-1} (\vec{\mu}-\vec{\bar{\mu}})$ is called **residual error**, sensory prediction error;
- $\vec{\varepsilon}_w = \Sigma_z^{-1} (\vec{\varphi}-g(\vec{\mu}))$ is called **prediction error**, model prediction error.

### Dynamical generative model

Definition table

|Symbol|Name|Description|
|:-|:-|:-|